{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c23516e2-d189-4592-89ef-0ebd6a77c277",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 06_Data_Quality\n",
    "\n",
    "from pyspark.sql.functions import col, count, expr\n",
    "\n",
    "# Set context\n",
    "spark.sql(\"USE CATALOG main\")\n",
    "spark.sql(\"USE SCHEMA ecommerce\")\n",
    "\n",
    "print(\"Starting Data Quality Checks...\")\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# CHECK 1: ROW COUNT CHECK (Are the tables empty?)\n",
    "# ---------------------------------------------------------\n",
    "tables_to_check = [\"silver_orders\", \"fact_sales\", \"gold_sales_by_state\"]\n",
    "\n",
    "for table in tables_to_check:\n",
    "    row_count = spark.read.table(table).count()\n",
    "    print(f\"Checking {table}: {row_count} rows found.\")\n",
    "    \n",
    "    if row_count == 0:\n",
    "        raise Exception(f\"CRITICAL ERROR: Table {table} is empty! Pipeline stopped.\")\n",
    "\n",
    "print(\"--> Row Count Checks Passed.\")\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# CHECK 2: NULL CHECK (Do we have Null Primary Keys?)\n",
    "# ---------------------------------------------------------\n",
    "# We check fact_sales for null order_ids\n",
    "null_count = spark.read.table(\"fact_sales\") \\\n",
    "    .filter(col(\"order_id\").isNull()) \\\n",
    "    .count()\n",
    "\n",
    "if null_count > 0:\n",
    "    raise Exception(f\"DATA INTEGRITY ERROR: Found {null_count} null Order IDs in Fact Table!\")\n",
    "\n",
    "print(\"--> Null ID Checks Passed.\")\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# CHECK 3: BUSINESS LOGIC CHECK (Negative Revenue?)\n",
    "# ---------------------------------------------------------\n",
    "# Revenue cannot be negative. If it is, something is wrong.\n",
    "negative_revenue_count = spark.read.table(\"fact_sales\") \\\n",
    "    .filter(col(\"revenue\") < 0) \\\n",
    "    .count()\n",
    "\n",
    "if negative_revenue_count > 0:\n",
    "    print(f\"WARNING: Found {negative_revenue_count} records with negative revenue.\")\n",
    "    # In some companies, this is a warning. In this assignment, let's be strict:\n",
    "    raise Exception(\"BUSINESS LOGIC ERROR: Negative Revenue found! Pipeline stopped.\")\n",
    "\n",
    "print(\"--> Revenue Sanity Checks Passed.\")\n",
    "\n",
    "print(\"------------------------------------------------\")\n",
    "print(\"SUCCESS: All Data Quality Checks Passed!\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "06_Data_Quality",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
