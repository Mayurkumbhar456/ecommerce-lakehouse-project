{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6c0b3de9-d64f-4832-9eb3-8e32db63b623",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 02_Silver_Layer\n",
    "\n",
    "from pyspark.sql.functions import col, to_timestamp\n",
    "\n",
    "# 1. Set the context to our catalog and schema\n",
    "spark.sql(\"USE CATALOG main\")\n",
    "spark.sql(\"USE SCHEMA ecommerce\")\n",
    "\n",
    "# Define path where bronze data sits\n",
    "bronze_base_path = \"/Volumes/main/ecommerce/lakehouse_vol/bronze/\"\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# TABLE 1: ORDERS (silver_orders)\n",
    "# ---------------------------------------------------------\n",
    "print(\"Processing: silver_orders...\")\n",
    "df_orders = spark.read.parquet(f\"{bronze_base_path}/orders/\")\n",
    "\n",
    "# Transformation:\n",
    "# 1. Convert string timestamp to actual Timestamp type (Critical for later steps)\n",
    "# 2. Drop duplicates on the Primary Key (order_id)\n",
    "df_orders_clean = df_orders \\\n",
    "    .withColumn(\"order_purchase_timestamp\", to_timestamp(col(\"order_purchase_timestamp\"))) \\\n",
    "    .dropDuplicates([\"order_id\"]) \\\n",
    "    .dropna(subset=[\"order_id\"]) # Drop if order_id is null\n",
    "\n",
    "# Write as Managed Delta Table\n",
    "df_orders_clean.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"silver_orders\")\n",
    "print(\"--> Created Table: main.ecommerce.silver_orders\")\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# TABLE 2: ORDER ITEMS (silver_order_items)\n",
    "# ---------------------------------------------------------\n",
    "print(\"Processing: silver_order_items...\")\n",
    "df_items = spark.read.parquet(f\"{bronze_base_path}/order_items/\")\n",
    "\n",
    "# Transformation: PK is order_id + order_item_id\n",
    "df_items_clean = df_items \\\n",
    "    .dropDuplicates([\"order_id\", \"order_item_id\"]) \\\n",
    "    .dropna(subset=[\"order_id\", \"order_item_id\"])\n",
    "\n",
    "df_items_clean.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"silver_order_items\")\n",
    "print(\"--> Created Table: main.ecommerce.silver_order_items\")\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# TABLE 3: CUSTOMERS (silver_customers)\n",
    "# ---------------------------------------------------------\n",
    "print(\"Processing: silver_customers...\")\n",
    "df_cust = spark.read.parquet(f\"{bronze_base_path}/customers/\")\n",
    "\n",
    "# Transformation: PK is customer_id\n",
    "df_cust_clean = df_cust \\\n",
    "    .dropDuplicates([\"customer_id\"]) \\\n",
    "    .dropna(subset=[\"customer_id\"])\n",
    "\n",
    "df_cust_clean.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"silver_customers\")\n",
    "print(\"--> Created Table: main.ecommerce.silver_customers\")\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# TABLE 4: PRODUCTS (silver_products)\n",
    "# ---------------------------------------------------------\n",
    "print(\"Processing: silver_products...\")\n",
    "df_prod = spark.read.parquet(f\"{bronze_base_path}/products/\")\n",
    "\n",
    "# Transformation: PK is product_id\n",
    "df_prod_clean = df_prod \\\n",
    "    .dropDuplicates([\"product_id\"]) \\\n",
    "    .dropna(subset=[\"product_id\"])\n",
    "\n",
    "df_prod_clean.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"silver_products\")\n",
    "print(\"--> Created Table: main.ecommerce.silver_products\")\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# TABLE 5: PAYMENTS (silver_payments)\n",
    "# ---------------------------------------------------------\n",
    "print(\"Processing: silver_payments...\")\n",
    "df_pay = spark.read.parquet(f\"{bronze_base_path}/payments/\")\n",
    "\n",
    "# Transformation: PK is order_id + payment_sequential\n",
    "df_pay_clean = df_pay \\\n",
    "    .dropDuplicates([\"order_id\", \"payment_sequential\"]) \\\n",
    "    .dropna(subset=[\"order_id\"])\n",
    "\n",
    "df_pay_clean.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"silver_payments\")\n",
    "print(\"--> Created Table: main.ecommerce.silver_payments\")\n",
    "\n",
    "print(\"------------------------------------------------\")\n",
    "print(\"SUCCESS: All Silver Managed Tables created!\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "02_Silver_Layer",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
