{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9f48d058-13e1-41a1-832c-4a18fbc564e9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 07_Time_Travel\n",
    "\n",
    "# Set context\n",
    "spark.sql(\"USE CATALOG main\")\n",
    "spark.sql(\"USE SCHEMA ecommerce\")\n",
    "\n",
    "# We will use 'gold_sales_by_state' for this demo because it is small (27 rows).\n",
    "target_table = \"gold_sales_by_state\"\n",
    "\n",
    "print(f\"--- 1. CURRENT STATE of {target_table} ---\")\n",
    "# Count rows before the disaster\n",
    "initial_count = spark.read.table(target_table).count()\n",
    "print(f\"Initial Row Count: {initial_count}\")\n",
    "\n",
    "\n",
    "# ====================================================\n",
    "# SIMULATE ACCIDENTAL DELETE\n",
    "# ====================================================\n",
    "print(\"\\n--- 2. SIMULATING ACCIDENTAL DELETE ---\")\n",
    "# \"Oops! I deleted all records for SP (Sao Paulo) state!\"\n",
    "spark.sql(f\"DELETE FROM {target_table} WHERE customer_state = 'SP'\")\n",
    "\n",
    "# Verify the damage\n",
    "after_delete_count = spark.read.table(target_table).count()\n",
    "print(f\"Row Count after DELETE: {after_delete_count}\")\n",
    "print(f\"Records lost: {initial_count - after_delete_count}\")\n",
    "\n",
    "\n",
    "# ====================================================\n",
    "# CHECK HISTORY\n",
    "# ====================================================\n",
    "print(\"\\n--- 3. CHECKING TABLE HISTORY ---\")\n",
    "df_history = spark.sql(f\"DESCRIBE HISTORY {target_table}\")\n",
    "display(df_history.select(\"version\", \"timestamp\", \"operation\", \"operationMetrics.numOutputRows\"))\n",
    "\n",
    "# We want to go back to the version BEFORE the delete.\n",
    "# The DELETE operation will be the latest version. We need (Latest Version - 1).\n",
    "last_version = df_history.first()[\"version\"]\n",
    "restore_version = last_version - 1\n",
    "\n",
    "print(f\"Current Version: {last_version} (This is the DELETE)\")\n",
    "print(f\"Restoring to Version: {restore_version} (This is the clean state)\")\n",
    "\n",
    "\n",
    "# ====================================================\n",
    "# RESTORE (TIME TRAVEL)\n",
    "# ====================================================\n",
    "print(f\"\\n--- 4. RESTORING TABLE TO VERSION {restore_version} ---\")\n",
    "spark.sql(f\"RESTORE TABLE {target_table} TO VERSION AS OF {restore_version}\")\n",
    "\n",
    "print(\"--> Restore Command Executed.\")\n",
    "\n",
    "\n",
    "# ====================================================\n",
    "# VALIDATE ROLLBACK\n",
    "# ====================================================\n",
    "print(\"\\n--- 5. VALIDATING ROLLBACK ---\")\n",
    "final_count = spark.read.table(target_table).count()\n",
    "print(f\"Final Row Count: {final_count}\")\n",
    "\n",
    "if final_count == initial_count:\n",
    "    print(\"SUCCESS: Data successfully restored! Magic confirmed.\")\n",
    "else:\n",
    "    print(\"ERROR: Row count does not match initial state.\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "07_Time_Travel",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
